{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fRkHvjNF7Bh"
      },
      "source": [
        "### **Introduction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDX9t9YxYsWV",
        "outputId": "fab1bb87-a427-4cd1-995d-38def2f5ead3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gradio --quiet\n",
        "!pip install tiktoken --quiet\n",
        "!pip install faiss-cpu --quiet\n",
        "!pip install unstructured[pdf] --quiet\n",
        "!pip install --upgrade openai --quiet\n",
        "!pip install --upgrade langchain --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VCBZhHIaU_z"
      },
      "source": [
        "### **Required Libs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8InvgB8YDLE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import gradio as gr\n",
        "\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import TokenTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsKUxAc2duLn"
      },
      "source": [
        "This is the API setup the **embeddings** and **chat** models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmeotZ1mEJ-z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2-QWilIcILB"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = \"29bc1acfcf004d14b50a7d3fb961ee11\"\n",
        "os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
        "os.environ['OPENAI_API_VERSION'] = \"2023-07-01-preview\"\n",
        "os.environ['OPENAI_API_BASE'] = \"https://c-openai-demo.openai.azure.com/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9iBzqHJAKeu"
      },
      "outputs": [],
      "source": [
        "def create_service_context(\n",
        "    # Constraint parameters\n",
        "    max_input_size=4096,        # Context window for the LLM.\n",
        "    num_outputs=256,            # Number of output tokens for the LLM.\n",
        "    chunk_overlap_ratio=0.1,    # Chunk overlap as a ratio of chunk size.\n",
        "    chunk_size_limit=None,      # Maximum chunk size to use.\n",
        "    chunk_overlap=20,           # Maximum chunk size to use.\n",
        "    chunk_size=1024,            # Set chunk overlap to use.\n",
        "):\n",
        "    # El código para configurar el contexto de servicio se mantiene aquí.\n",
        "\n",
        "    # The parser that converts documents into nodes.\n",
        "    node_parser = SimpleNodeParser.from_defaults(\n",
        "        # The text splitter used to split text into chunks.\n",
        "        text_splitter=TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    )\n",
        "\n",
        "    # Allows the user to explicitly set certain constraint parameters.\n",
        "    prompt_helper = PromptHelper(\n",
        "        max_input_size,\n",
        "        num_outputs,\n",
        "        chunk_overlap_ratio,\n",
        "        chunk_size_limit=chunk_size_limit)\n",
        "\n",
        "    # LLMPredictor is a wrapper class around LangChain's LLMChain that allows easy integration into LlamaIndex.\n",
        "    llm_predictor = LLMPredictor(\n",
        "        llm=AzureChatOpenAI(\n",
        "            #temperature=0.5,\n",
        "            deployment_name=\"chagpt_model\",\n",
        "            max_tokens=num_outputs))\n",
        "\n",
        "    # The embedding model used to generate vector representations of text.\n",
        "    embedding_llm = LangchainEmbedding(\n",
        "        langchain_embeddings=OpenAIEmbeddings(\n",
        "            model=\"text-embedding-ada-002\",\n",
        "            chunk_size=1)\n",
        "    )\n",
        "\n",
        "    # Constructs service_context\n",
        "    service_context = ServiceContext.from_defaults(\n",
        "        llm_predictor=llm_predictor,\n",
        "        embed_model=embedding_llm,\n",
        "        node_parser=node_parser,\n",
        "        prompt_helper=prompt_helper)\n",
        "\n",
        "    return service_context\n",
        "\n",
        "# Función para obtener respuestas del modelo de lenguaje de OpenAI en español\n",
        "def get_response(input_text):\n",
        "    # Crear una instancia del modelo de lenguaje de OpenAI en español (o el idioma deseado)\n",
        "    model = pipeline(\"text-generation\", model=\"text-embedding-ada-002\", device=0, max_length=50)  # Asegúrate de usar el modelo correcto aquí\n",
        "\n",
        "    # Obtener una respuesta del modelo\n",
        "    response = model(input_text, num_return_sequences=1)\n",
        "\n",
        "    # Devolver la respuesta generada por el modelo\n",
        "    return response[0]['generated_text']\n",
        "    return service_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTMA-koTGU94"
      },
      "source": [
        "### **Getting Started**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QCISV9OGr76"
      },
      "source": [
        "At a high level Walkthrough, there are two components to setting up ChatGPT over your own data:\n",
        "1. `Ingestion of the data`\n",
        "2. `Chatbot over the data`\n",
        "\n",
        "Walking through the steps of each at a high level in the upcoming section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3UX1w4pGcRH"
      },
      "source": [
        "#### **Data Ingestion**\n",
        "This section dives into more detail on the steps necessary to ingest data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV6lACjthOZM"
      },
      "source": [
        "Next, we can load up a bunch of text files, chunk them up and embed them. LangChain supports a lot of different [document loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders.html), which makes it easy to adapt to other data sources and file formats. You can download the sample data here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNxEY_2PSlea"
      },
      "outputs": [],
      "source": [
        "# Initialize gpt-35-turbo and our embedding model\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=\"chagpt_model\",\n",
        "    openai_api_version=\"2023-03-15-preview\")\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    chunk_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpEMQeAcTqER"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiBJ2wUOTk7J",
        "outputId": "4cb2a8cd-e39d-4a72-a24a-105648cf44b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "loader = DirectoryLoader('data',\n",
        "                         glob=\"*.pdf\",\n",
        "                         #loader_cls=TextLoader\n",
        "                         )\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRdKisd5hdZQ"
      },
      "source": [
        "Next, let's ingest documents into [Faiss](https://github.com/facebookresearch/faiss) so we can efficiently query our embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqOkBzS6T0Gw"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(documents=docs, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjNgXBeNJl19"
      },
      "source": [
        "#### **Data Querying**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3yo_CRHVeSZ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Adapt if needed\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"HIBuddy Bot ha sido diseñado específicamente para responder a las preguntas y brindar orientación a los empleados de Hiberus,\n",
        "tanto en sus primeros días en la empresa como en las dudas que puedan surgir en su rutina diaria.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\")\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "                                           retriever=db.as_retriever(),\n",
        "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
        "                                           return_source_documents=True,\n",
        "                                           verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "fXh27AepE_w1",
        "outputId": "7f3c0a88-45fc-4ad8-e54c-b016ad3d9b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Quiero cambiar tipo de IRPF, ¿es posible?\n",
            "Answer:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sí, es posible cambiar el tipo de IRPF. Debes enviar una Solicitud de tipo de retención superior, cumplimentada y firmada, a RRHH para solicitar el cambio. Sin embargo, este cambio solo se puede realizar si el tipo solicitado es superior al que resulta del cálculo realizado por nuestro sistema.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history = []\n",
        "query = \"Quiero cambiar tipo de IRPF, ¿es posible?\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(\"Question:\", query)\n",
        "print(\"Answer:\")\n",
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fYLV6SPBViJM",
        "outputId": "865e83db-6bd8-4e07-b79e-d90e7fe449ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Dónde puedo ver mi calendario laboral corporativo?\n",
            "Answer:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Puedes ver y descargar tu calendario laboral en Sommos, Área personal → Mi jornada conciliación.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history = []\n",
        "query = \"Dónde puedo ver mi calendario laboral corporativo?\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(\"Question:\", query)\n",
        "print(\"Answer:\")\n",
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "-nJMQGnjWk8H",
        "outputId": "be7bd654-2d18-445f-d350-e2556d7a458d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 5 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: ¿Cómo puedo descargar la app Sommos?\n",
            "Answer:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Puede acceder a la app Sommos desde https://sommos.online/hiberus/ o descargarla en su móvil. Las instrucciones para descargarla se encuentran en el manual adjunto en la convocatoria del Onboarding.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Follow-up questions support\n",
        "chat_history.append((query, result[\"answer\"]))\n",
        "chat_history=[]\n",
        "query = \"¿Cómo puedo descargar la app Sommos?\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(\"Question:\", query)\n",
        "print(\"Answer:\")\n",
        "\n",
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRKEXicc_HeW"
      },
      "outputs": [],
      "source": [
        "async def data_querying(input_text, follow_up_questions = True):\n",
        "  #Reconstruir el storage context\n",
        "  storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "\n",
        "  #Carga el índice de almacenamiento\n",
        "  Index = load_index_from_storage(storage_context, service_context=create_service_context())\n",
        "\n",
        "  #Cromprueba si es un chat de seguimiento o no\n",
        "  #A continuación, consulta el índice con el texto de entrada\n",
        "  if follow_up_questions:\n",
        "    response = index.as_chat_engine().chat(input_text)\n",
        "  else:\n",
        "    response = index.as_query_engine().query(input_text)\n",
        "  return response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "45Lv074I-wS-",
        "outputId": "e804b1d8-f6f9-4493-eb14-73a3db4f0fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://1b85de3462dabed7bc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://1b85de3462dabed7bc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:  Invalid HTTP request received.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 507, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 219, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1437, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1107, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 616, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\", line 415, in _submit_fn\n",
            "    response = await self.fn(message, history, *args)\n",
            "  File \"<ipython-input-27-f408e7b3507b>\", line 3, in data_querying\n",
            "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
            "NameError: name 'StorageContext' is not defined\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 507, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 219, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1437, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1107, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 616, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\", line 415, in _submit_fn\n",
            "    response = await self.fn(message, history, *args)\n",
            "  File \"<ipython-input-27-f408e7b3507b>\", line 3, in data_querying\n",
            "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
            "NameError: name 'StorageContext' is not defined\n"
          ]
        }
      ],
      "source": [
        "# Inferfaz gradio\n",
        "iface = gr.ChatInterface(\n",
        "    data_querying,\n",
        "    chatbot=gr.Chatbot(height=300),\n",
        "    textbox=gr.Textbox(placeholder=\"¿Qué quieres saber acerca de Hiberus?\", container=False, scale=7),\n",
        "    title=\"HIBuddy Bot\",\n",
        "    description=\"Soy el Buddy de Hiberus, y estoy encantado de poder ayudarte en tus primeros días en la empresa\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\"¿Cómo puedo descargar la app Sommos?\", \"Quiero cambiar tipo de IRPF, ¿es posible?\", \"¿Cuando debo solicitar mis vacaciones?\"],\n",
        "    cache_examples=False,\n",
        "    retry_btn=\"Repetir\",\n",
        "    undo_btn=\"Deshacer\",\n",
        "    clear_btn=\"Borrar\",\n",
        "    submit_btn=\"Enviar\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}