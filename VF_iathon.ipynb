{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMN55acT75M0cakHTs42evu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asanchezmh/iathon_vf/blob/main/VF_iathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Installations**\n",
        "\n",
        "!mkdir data\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "!pip install pypdf --quiet\n",
        "!pip install gradio --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install llama_index --quiet\n",
        "!pip install googletrans --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install docx2txt --quiet\n",
        "\n",
        "### **Imports**\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "\n",
        "from llama_index import (\n",
        "    SimpleDirectoryReader,\n",
        "    LLMPredictor,\n",
        "    PromptHelper,\n",
        "    StorageContext,\n",
        "    ServiceContext,\n",
        "    GPTVectorStoreIndex,\n",
        "    LangchainEmbedding,\n",
        "    load_index_from_storage,\n",
        "    set_global_service_context)\n",
        "\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index.text_splitter import TokenTextSplitter\n",
        "from llama_index.response.notebook_utils import display_response\n",
        "\n",
        "### **Azure OpenAI**\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"29bc1acfcf004d14b50a7d3fb961ee11\"\n",
        "os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
        "os.environ['OPENAI_API_VERSION'] = \"2023-07-01-preview\"\n",
        "os.environ['OPENAI_API_BASE'] = \"https://c-openai-demo.openai.azure.com/\"\n",
        "\n",
        "# **Service Context**\n",
        "\n",
        "def create_service_context(\n",
        "    # Constraint parameters\n",
        "    max_input_size=4096,        # Context window for the LLM.\n",
        "    num_outputs=256,            # Number of output tokens for the LLM.\n",
        "    chunk_overlap_ratio=0.1,    # Chunk overlap as a ratio of chunk size.\n",
        "    chunk_size_limit=None,      # Maximum chunk size to use.\n",
        "    chunk_overlap=20,           # Maximum chunk size to use.\n",
        "    chunk_size=1024,            # Set chunk overlap to use.\n",
        "):\n",
        "    # The code to configure the service context is moved to a separate function.\n",
        "\n",
        "    def _configure_service_context():\n",
        "        node_parser = SimpleNodeParser.from_defaults(\n",
        "            text_splitter=TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        )\n",
        "        prompt_helper = PromptHelper(\n",
        "            max_input_size,\n",
        "            num_outputs,\n",
        "            chunk_overlap_ratio,\n",
        "            chunk_size_limit=chunk_size_limit)\n",
        "        llm_predictor = LLMPredictor(\n",
        "            llm=AzureChatOpenAI(\n",
        "                #temperature=0.5,\n",
        "                deployment_name=\"chagpt_model\",\n",
        "                max_tokens=num_outputs))\n",
        "        embedding_llm = LangchainEmbedding(\n",
        "            langchain_embeddings=OpenAIEmbeddings(\n",
        "                model=\"text-embedding-ada-002\",\n",
        "                chunk_size=1)\n",
        "        )\n",
        "        service_context = ServiceContext.from_defaults(\n",
        "            llm_predictor=llm_predictor,\n",
        "            embed_model=embedding_llm,\n",
        "            node_parser=node_parser,\n",
        "            prompt_helper=prompt_helper)\n",
        "        return service_context\n",
        "\n",
        "    return _configure_service_context()\n",
        "\n",
        "# Función para obtener respuestas del modelo de lenguaje de OpenAI en español\n",
        "def get_response(input_text):\n",
        "    # Crear una instancia del modelo de lenguaje de OpenAI en español (o el idioma deseado)\n",
        "    model = pipeline(\"text-generation\", model=\"text-embedding-ada-002\", device=0, max_length=50)  # Asegúrate de usar el modelo correcto aquí\n",
        "\n",
        "    # Obtener una respuesta del modelo\n",
        "    response = model(input_text, num_return_sequences=1)\n",
        "\n",
        "    # Devolver la respuesta generada por el modelo\n",
        "    return response[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rrt0Nu1EIc0T",
        "outputId": "e8e823eb-8141-4184-a976-728554d0f473"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.0/276.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m790.7/790.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = create_service_context()\n",
        "set_global_service_context(service_context)"
      ],
      "metadata": {
        "id": "IRwP3fFpJ8mv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_ingestion_indexing(data):\n",
        "  #Carga los datos de la ruta de directorio especificada\n",
        "  documents = SimpleDirectoryReader ('data').load_data()\n",
        "\n",
        "  #Al crear el indice por primera vez\n",
        "  index = GPTVectorStoreIndex.from_documents(\n",
        "      documents, service_context=create_service_context()\n",
        "  )\n",
        "\n",
        "  #Manten el índice en el disco, carpeta \"storage\" por defecto\n",
        "  index.storage_context.persist()\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "albK_8tVJxlQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = data_ingestion_indexing (\"data\")"
      ],
      "metadata": {
        "id": "8KajXePZJ9p2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def data_querying(input_text, follow_up_questions = True):\n",
        "  #Reconstruir el storage context\n",
        "  storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "\n",
        "  #Carga el índice de almacenamiento\n",
        "  Index = load_index_from_storage(storage_context, service_context=create_service_context())\n",
        "\n",
        "  #Cromprueba si es un chat de seguimiento o no\n",
        "  #A continuación, consulta el índice con el texto de entrada\n",
        "  if follow_up_questions:\n",
        "    response = index.as_chat_engine().chat(input_text)\n",
        "  else:\n",
        "    response = index.as_query_engine().query(input_text)\n",
        "  return response.response"
      ],
      "metadata": {
        "id": "FvDyEeLVJpr3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para mostrar el historial de respuestas\n",
        "def mostrar_historial():\n",
        "    historial_texto = \"\\n\".join(historial_respuestas)\n",
        "    return historial_texto\n",
        "\n",
        "# Define la URL de la imagen del logo\n",
        "logo_url = \"https://www.hiberus.com/sites/default/files/2022-02/Logo_Hiberus_Azul_0.png\"\n",
        "\n",
        "iface = gr.ChatInterface(\n",
        "    data_querying,\n",
        "    chatbot=gr.Chatbot(height=300),\n",
        "    textbox=gr.Textbox(placeholder=\"¿Qué quieres saber acerca de Hiberus?\", container=False, scale=7),\n",
        "    title=\"HIBuddy Bot\",\n",
        "    description=\"Soy el Buddy de Hiberus, y estoy encantado de poder ayudarte en tus primeros días en la empresa\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\"¿Cómo puedo descargar la app Sommos?\", \"Quiero cambiar tipo de IRPF, ¿es posible?\", \"¿Cuando debo solicitar mis vacaciones?\"],\n",
        "    cache_examples=True,\n",
        "    retry_btn=\"Repetir\",\n",
        "    undo_btn=\"Deshacer\",\n",
        "    clear_btn=\"Borrar\",\n",
        "    submit_btn=\"Enviar\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True, debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "YG5pgvilJkTO",
        "outputId": "3348c6d7-fa18-4a70-9977-49a5bb487463"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching examples at: '/content/gradio_cached_examples/17'\n",
            "Caching example 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-07-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 8 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching example 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-07-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 4 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-07-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 9 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching example 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-07-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 5 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-07-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 1 second. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching complete\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://21302de2e1d4174f57.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://21302de2e1d4174f57.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}